= Exploring Spring Batch Framework
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2023-12
// Metadata
:description: This repository contains an example of how to run Batch processes on OpenShift using Argo Workflows.
:keywords: openshift, red hat, Batch, Argo, workflows, Spring
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]
// End: Enable admonition icons


This repository contains an example of how to run Batch processes on OpenShift using Argo Workflows.

// Create the Table of contents here
toc::[]

== Introduction


This demo contains all you need to deploy Argo Workflows to coordinate the deployment and execution of batch processes created using Spring Batch. For that reason, this repository contains the following sections:

* First, a quick mechanism to deploy **ArgoCD**. https://argoproj.github.io/cd[Argo CD] automates the deployment of the desired application states in the specified target environments. We will use it to deploy Argo Workflows and our Spring Batch application.
* Second, **Argo Workflows**. https://argoproj.github.io/workflows[Argo Workflows] is a container-native workflow engine for orchestrating parallel jobs on Kubernetes. We will use it to handle the execution of the batch processes.
* Third, **Spring Batch**. https://spring.io/projects/spring-batch[Spring Batch] is a processing framework designed for robust execution of jobs. We will use it to load data from a CSV file, process it, and store it in a Database deployed on OpenShift.

NOTE: All of this will be deployed and automated on OpenShift using the GitOps principles. If you don't have an OpenShift cluster, https://github.com/alvarolop/ocp-installation[check this repo] on how to deploy an OCP cluster on-premise.

TIP: `Data Processing` is one of the https://argoproj.github.io/argo-workflows/use-cases/data-processing/[typical use cases] for Argo Workflows.


== ArgoCD 

There are many ways to install an ArgoCD instance. Among all of them, I've chosen to use https://docs.openshift.com/gitops/1.11/understanding_openshift_gitops/about-redhat-openshift-gitops.html[Red Hat's OpenShift GitOps]. Red Hat OpenShift GitOps is based on the open source project https://argo-cd.readthedocs.io/en/stable/[Argo CD] and provides a similar set of features to what the upstream offers, with additional automation, integration into Red Hat OpenShift Container Platform and the benefits of Red Hat's enterprise support, quality assurance and focus on enterprise security.


The installation of OpenShift GitOps is based on an operator that is installed using the Operator Lifecycle Manager (OLM). To simplify the installation of the operator and the creation of the ArgoCD instance, I've created a simple script that you can run on a clean OCP cluster:

[source, bash]
----
./10-argocd/auto-install.sh
----




== Argo Workflows


There are, also, many official ways to deploy Argo Workflows, all of them on https://argoproj.github.io/argo-workflows/installation/[this page of the documentation]. Apart from the quick-start installation, there are two mechanisms, the single-YAML release manifests and the community-maintained Helm Charts. For this exercise, we are going to use the Helm Charts, as it provides more flexibility and adaptability. Also, we are going to use an ArgoCD application to deploy the chart on OpenShift.


[source, bash]
----
./20-argo-workflows/auto-install.sh
----

From the basic Helm Chart provided in their https://github.com/argoproj/argo-helm/tree/main/charts/argo-workflows[GitHub repository], I've also added both a Namespace and a Route to make sure that the resources are not deployed in the `default` namespace.

After that, I have created a modified `values.yaml` to customize the Argo Workflows behavior:

* Provide HA configuration, based on the https://argoproj.github.io/argo-workflows/high-availability/[documentation] and the https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/ci/ha-values.yaml[Helm Chart example values].
* Expose metrics and collect them using a ServiceMonitor according to the https://argoproj.github.io/argo-workflows/metrics/[documentation]. The Helm chart already takes care of that in these https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/ci/enable-metrics-values.yaml[example values].


NOTE: For general info, I recommend this guide about https://blog.argoproj.io/practical-argo-workflows-hardening-dd8429acc1ce[Argo Workflows Hardening].


== Spring Batch application


Manual steps to generate the container image locally:

[source, bash]
----
# Generate the Jar file with all the dependencies
mvn clean package

# Add the executable to a container image
podman build -f src/main/docker/Dockerfile.springboot-jar -t spring-boot/spring-batch-capitalator .

# Launch the application
podman run -i --rm spring-boot/spring-batch-capitalator
----

=== Push image to Quay


[source, bash]
----
# Tag the image to point to your Quay URL
podman tag spring-boot/spring-batch-capitalator quay.io/alopezme/spring-batch-capitalator

# Push image to Quay
podman push quay.io/alopezme/spring-batch-capitalator
----



== GitHub Actions



== PostgreSQL Database




== Useful resources



